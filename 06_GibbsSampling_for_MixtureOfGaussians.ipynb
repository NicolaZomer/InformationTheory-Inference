{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of Gibbs Sampling to Mixture of Gaussians\n",
    "\n",
    "## Task\n",
    "The file `data_ex06.csv` contains data sampled from a mixture of $K = 4$ Gaussians. The standard deviations of the Gaussians are known and\n",
    "identical, $\\sigma_k = \\sigma = 0.5$. Reconstruct the unknown parameters and the latent variables:\n",
    "$$\n",
    "    p_k, \\mu_k, Z_i\n",
    "$$\n",
    "where $Z_i$ represents the assignment of each point to one of the mixture components. To perform the inference, use _Gibbs sampling_.\n",
    "\n",
    "\n",
    "## Gibbs sampling\n",
    "**Step 1** <br>\n",
    "Initialize randomly $p_k, \\mu_k, Z_i$. Compute $N_k = \\sum_i \\chi(Z_i=k)$ and $m_k=\\sum_i \\chi(Z_i=k) x_k$.\n",
    "\n",
    "**Step 2** <br>\n",
    "Perform Gibbs sampling iterations:\n",
    "- Sample $\\mu_k$ for all $k$ from a normal with mean:\n",
    "  $$\n",
    "    \\mu'_k = \\left(\\frac{\\mu_0}{\\sigma_0^2}+\\frac{m_k}{\\sigma^2}\\right)\\frac{1}{\\left(\\frac{1}{\\sigma_0^2}+\\frac{N_k}{\\sigma^2}\\right)}\n",
    "  $$\n",
    "  and standard deviation:\n",
    "  $$\n",
    "    \\sigma'_k = \\left(\\frac{1}{\\sigma_0^2}+\\frac{N_k}{\\sigma^2}\\right)^{-\\frac{1}{2}}\n",
    "  $$\n",
    "  Here $\\mu_0 = 0$ and $\\sigma_0 = 1000$ are the Gaussian prior parameters. Do this for all $k$.\n",
    "\n",
    "- Sample the $\\vec{p}$ from a Dirichlet distribution with parameters $\\gamma'_k=\\gamma_k+N_k$. Here $\\gamma_k=1$ are the Dirichlet prior parameters.\n",
    "\n",
    "- Sample the $Z_i$ from a categorical distribution:\n",
    "  $$\n",
    "    P(Z_i=k) = \\frac{p_k\\exp\\left\\{-\\frac{(x_k-\\mu_k)^2}{2\\sigma^2}\\right\\}}{\\sum_k p_k\\exp\\left\\{-\\frac{(x_k-\\mu_k)^2}{2\\sigma^2}\\right\\}}\n",
    "  $$\n",
    "\n",
    "- If $Z_i$ is updated, update $N_k$ and $m_k$, and consequentially $\\vec{\\mu}', \\vec{\\sigma}', \\vec{\\gamma}', \\vec{p}'$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture of Gaussians (MoG)\n",
    "Assume to have $N$ 1-dimensional points generated from a Mixture of $D$ Gaussians with parameters $\\mu_i, \\sigma_i$ for $i=1, \\dots, N$. Then, assuming i.i.d. points, the likelihood function can be written as:\n",
    "$$\n",
    "  f\\left(\\vec{x}|\\vec{p}, \\vec{\\mu}, \\vec{\\sigma}\\right) = \\prod_{i=1}^N f\\left(x_i|\\vec{p}, \\vec{\\mu}, \\vec{\\sigma}\\right) =  \n",
    "  \\prod_{i=1}^N \\sum_{k=1}^D p_k \\mathcal{N}(x_i|\\mu_k, \\sigma_k^2)\n",
    "$$\n",
    "Given the likelihood, the posterior becomes:\n",
    "$$\n",
    "  f\\left(\\vec{p}, \\vec{\\mu}, \\vec{\\sigma}|\\vec{x}\\right) = \\left(\\prod_{i=1}^N \\sum_{k=1}^D p_k \\mathcal{N}(x_i|\\mu_k, \\sigma_k^2)\\right) \\times f_{prior}(\\vec{p}, \\vec{\\mu}, \\vec{\\sigma})\n",
    "$$\n",
    "\n",
    "The problem with respect to Metropolis is that the order of operations involved in the computation of the likelihood is $\\mathcal{O}(DN)$, which is computationally heavy and makes the problem intractable. \n",
    "\n",
    "A strategy to simplify the problem and make the inference task more interesting is to write the generative process in a slightly different form, i.e. as the combination of 2 random processes:\n",
    "1. extract $Z_i\\in\\{1, \\dots, D\\}$ with probability $\\{p_1, \\dots, p_D\\}$\n",
    "2. extract $x_i$ from the corresponding component of the mixture: $\\mathcal{N}(\\mu_i, \\sigma_i^2)$\n",
    "\n",
    "The variables $Z_i$ are called _latent variables_ and are unknown. They represent the mixture component from which each point $x_i$ was sampled. Thus, they are distributed according to a categorical distribution:\n",
    "$$\n",
    "  Z_i\\sim Cat(\\vec{p}) \\quad \\text{i.e.} \\quad P(Z_i=k)=p_k\n",
    "$$\n",
    "Then the likelihood becomes:\n",
    "$$\n",
    "  f\\left(x_i|\\vec{\\mu}, \\vec{\\sigma}, Z_i\\right) = \\mathcal{N}(\\mu_{Z_i}, \\sigma_{Z_i}^2) \\iff f\\left(x_i|\\vec{\\mu}, \\vec{\\sigma}, Z_i=k\\right) = \\mathcal{N}(\\mu_k, \\sigma_k^2)\n",
    "$$\n",
    "meaning that when $Z_i = k$,  $x_i$ is sampled from a Gaussian with mean $\\mu_k$ and variance $\\sigma^2$. \n",
    "\n",
    "We can also rewrite the posterior using the latent variables:\n",
    "$$\n",
    "\\begin{align*}\n",
    "  f\\left(\\vec{x}|\\vec{\\mu}, \\vec{\\sigma}, \\vec{Z}\\right) &= \\prod_{i=1}^N f\\left(x_i|\\vec{\\mu}, \\vec{\\sigma}, Z_i\\right) = \\\\\n",
    "  &= \\prod_{i=1}^N \\mathcal{N}(x_i|\\mu_{Z_i}, \\sigma_{Z_i}^2) = \\\\\n",
    "  &= \\prod_{i=1}^N \\left(2\\pi\\sigma_{Z_i}^2\\right)^{-\\frac{1}{2}} e^{-\\frac{1}{2}\\frac{(x_i-\\mu_{Z_i})^2}{\\sigma_{Z_i}^2}} = \\\\\n",
    "  &= \\prod_{k=1}^D \\left(2\\pi\\sigma_{Z_i}^2\\right)^{-\\frac{N_k}{2}} e^{-\\frac{1}{2}\\sum_{i=1}^N \\chi(Z_i=k)\\frac{(x_i-\\mu_k)^2}{\\sigma_{k}^2}}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\chi(Z_i=k)=1$ if $x_i$ is such that $Z_i=k$, else $0$. Here $N_k = \\sum_i \\chi(Z_i=k)$.\n",
    "\n",
    "Notice that the model with latent variables $Z_i$ allows to:\n",
    "1. Drastically reduce the computational complexity of computing the likelihood\n",
    "2. Use conjugate priors, which considerably simplifies the shape of the posterior, enabling Gibbs sampling\n",
    "   \n",
    "At this point we can infer all the parameters from the data, assuming for simplicity that $\\sigma_k=\\sigma=0.5 \\, \\forall k$. \n",
    "\n",
    "### Priors\n",
    "- The prior of $\\mu_k$ is Gaussian with parameters $\\mu_0$, $\\sigma_0$: \n",
    "  $$ \n",
    "    \\mu_k|\\mu_0, \\sigma_0 \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)\n",
    "  $$\n",
    "  Then:\n",
    "  $$ \n",
    "    f_{prior}(\\vec{\\mu}|\\mu_0, \\sigma_0^2)=\\prod_{k=1}^D f_{prior}(\\mu_k|\\mu_0, \\sigma_0^2) = (2\\pi\\sigma_{0})^{-\\frac{D}{2}}\\prod_{k=1}^D e^{-\\frac{(\\mu_0-\\mu_k)^2}{2\\sigma_0^2}}\n",
    "  $$\n",
    "\n",
    "- For $\\vec{p}$ we may assume a Dirichlet prior, which is the generalization of a Beta, i.e. the conjugate prior for Bernoulli trials:\n",
    "  $$\n",
    "    p_{k}|\\vec{\\gamma}\\sim Dir(\\vec{\\gamma})\n",
    "  $$ \n",
    "  or \n",
    "  $$\n",
    "    f_{prior}(\\vec{p}|\\vec{\\gamma})=(\\beta(\\vec{\\gamma}))^{-1}\\prod_{k=1}^D p_{k}^{\\gamma_{k}-1}\n",
    "  $$\n",
    "  This is the conjugate prior for the multinomial distribution. We can fix the hyperparameters to have a non-informative prior $\\gamma_{k}=1$\n",
    "  (uniform prior). \n",
    "\n",
    "- From the assumptions, we know that \n",
    "  $$\n",
    "    f_{prior}(Z_i=k|p_k)=p_k \\quad \\Longrightarrow \\quad f_{prior}(\\vec{Z}|\\vec{p}) = \\prod_{k=1}^D p_k^{N_k} \n",
    "  $$\n",
    "\n",
    "### Posterior\n",
    "The posterior simply becomes:\n",
    "$$\n",
    "f(\\vec{\\mu},\\vec{Z},\\vec{p}|\\vec{x})= \\frac{1}{E(\\vec{x})}\\times f_{likelihood}(\\vec{x}|\\vec{\\mu},\\sigma, \\vec{Z})\\times f_{prior}(\\vec{\\mu}|\\mu_{0}, \\sigma_{0})\\times f_{prior}(\\vec{Z}|\\vec{p})\\times f_{prior}(\\vec{p}|\\vec{\\gamma})\n",
    "$$\n",
    "where $E(\\vec{x})$ is the evidence. \n",
    "\n",
    "### Conditional posteriors\n",
    "We want to sample the above posterior using Gibbs Sampling. To do so, we need to compute the conditional distributions:\n",
    "$$\n",
    "\\begin{align*}\n",
    "  f&(\\mu_k | \\vec{\\mu}_{-k},\\vec{Z},\\vec{p}, \\vec{x}) \\\\\n",
    "  f&(p_k | \\vec{\\mu},\\vec{Z},\\vec{p}_{-k}, \\vec{x}) \\\\\n",
    "  f&(Z_i | \\vec{\\mu},\\vec{Z}_{-i},\\vec{p}, \\vec{x}) \n",
    "\\end{align*}\n",
    "$$\n",
    "Even if it looks very difficult, in the computation most terms cancel out. The computation is done using Bayes theorem and marginalization; for semplicity we report here only the final results. \n",
    "\n",
    "For $\\mu_k$:\n",
    "$$\n",
    "  \\mu_{k}|\\vec{\\mu}_{-k},\\vec{Z},\\vec{p}, \\vec{x}\\sim \\mathcal{N}(\\mu_k', \\sigma_k'^2)\n",
    "$$\n",
    "with\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\mu'_k &= \\left(\\frac{\\mu_0}{\\sigma_0^2}+\\frac{m_k}{\\sigma^2}\\right)\\frac{1}{\\left(\\frac{1}{\\sigma_0^2}+\\frac{N_k}{\\sigma^2}\\right)} \\\\\n",
    "  \\sigma'_k &= \\left(\\frac{1}{\\sigma_0^2}+\\frac{N_k}{\\sigma^2}\\right)^{-\\frac{1}{2}}\n",
    "\\end{align*}  \n",
    "$$\n",
    "\n",
    "For $p_k$:\n",
    "$$\n",
    "  \\vec{p}|\\vec{\\mu},\\vec{Z},\\vec{p}_{-k}, \\vec{x}\\sim Dir(\\vec{\\gamma'}) \n",
    "$$\n",
    "with\n",
    "$$\n",
    "  \\vec{\\gamma'}=\\vec{\\gamma}+\\vec{N}\n",
    "$$\n",
    "\n",
    "Finally, for $Z_i$:\n",
    "$$\n",
    "  Z_{i}|\\vec{\\mu},\\vec{Z}_{-i},\\vec{p}, \\vec{x} \\sim Cat(\\vec{p}')\n",
    "$$\n",
    "with\n",
    "$$\n",
    "  {p'}_k \\propto p_k e^{-\\frac{(x_{i}-\\mu_{k})^{2}}{2\\sigma^{2}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dabd4eacb9e3ddd54eefcebcf933935be170cfdef29c95630fe9376e024beb9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
